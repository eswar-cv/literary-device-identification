{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UE20CS334 - Natural Language Processing - Project\n",
        "## Team 03\n",
        "## Literary device Identification - Personification\n",
        "\n",
        "### Team Members\n",
        "\n",
        "| Name                  | SRN           |\n",
        "| --------------------- | ------------- |\n",
        "| Ajay Anil Kumar       | PES2UG20CS028 |\n",
        "| C V Eswar Sai Reddy   | PES2UG20CS096 |\n",
        "| Rudra Narayan Samanta | PES2UG20CS286 |"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W_Qz7SO76Eiw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "MOH_train = pd.read_csv(\"CLS/train0.tsv\",sep='\\t',header=0,names = ['index'\t,'label'\t,'sentence'\t,'pos'\t,'v_index'])\n",
        "MOH_test = pd.read_csv(\"CLS/test0.tsv\",sep='\\t',header=0,names = ['index'\t,'label'\t,'sentence'\t,'pos'\t,'v_index'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAPTEoqlr8UF",
        "outputId": "6cc70578-7a23-4290-91b1-37f16e3dc3ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\cvesw\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\cvesw\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\cvesw\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import tensorflow as tf\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from tensorflow import keras\n",
        "from keras import regularizers, optimizers\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Activation, Dropout, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import fasttext\n",
        "from sklearn.svm import SVC\n",
        "import pickle\n",
        "ft_model = fasttext.load_model(\"cc.en.300.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.concat([MOH_train])\n",
        "test_df = pd.concat([MOH_test])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tmUHFXPl90h5"
      },
      "outputs": [],
      "source": [
        "def preprocess(dataset):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for j,i in enumerate(dataset[\"sentence\"]):\n",
        "    word_tokens = word_tokenize(i)\n",
        "    filtered_sentence = [lemmatizer.lemmatize(w.lower()) for w in word_tokens if not w.lower() in stop_words and w.isalpha() and len(w)>2]\n",
        "    dataset[\"sentence\"][j] = filtered_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\cvesw\\AppData\\Local\\Temp\\ipykernel_33632\\1281584374.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"sentence\"][j] = filtered_sentence\n"
          ]
        }
      ],
      "source": [
        "train_df = train_df.drop_duplicates(subset=[\"sentence\",\"label\"], keep='first')\n",
        "train_df = train_df.drop_duplicates(subset=[\"sentence\"], keep='last')\n",
        "train_df = train_df.reset_index(level=0, drop=True, inplace=False, col_level=0, col_fill='')\n",
        "preprocess(train_df)\n",
        "\n",
        "test_df = test_df.drop_duplicates(subset=[\"sentence\",\"label\"], keep='first')\n",
        "test_df = test_df.drop_duplicates(subset=[\"sentence\"], keep='last')\n",
        "test_df = test_df.reset_index(level=0, drop=True, inplace=False, col_level=0, col_fill='')\n",
        "preprocess(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = train_df.dropna(subset=['sentence'])\n",
        "train_df = train_df.reset_index(level=0, drop=True, inplace=False, col_level=0, col_fill='')\n",
        "test_df = test_df.dropna(subset=['sentence'])\n",
        "test_df = test_df.reset_index(level=0, drop=True, inplace=False, col_level=0, col_fill='')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Converting Words to Fasttext Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_features = ft_model.get_dimension()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_to_vector(listOfWords):\n",
        "    listOfVectors = []\n",
        "    for i in listOfWords:\n",
        "        x = np.zeros((n_features))\n",
        "        x = ft_model.get_word_vector(i).astype('float32')\n",
        "        listOfVectors.append(x)\n",
        "    \n",
        "    while(len(listOfVectors) < 8):\n",
        "        x = np.zeros((n_features))\n",
        "        listOfVectors.append(x)\n",
        "\n",
        "    listOfVectors = np.array(listOfVectors)\n",
        "    return listOfVectors\n",
        "\n",
        "def dataset_to_vector(dataset):\n",
        "    for j,i in enumerate(dataset[\"sentence\"]):\n",
        "        dataset[\"sentence\"][j] = text_to_vector(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\cvesw\\AppData\\Local\\Temp\\ipykernel_33632\\61044175.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"sentence\"][j] = text_to_vector(i)\n"
          ]
        }
      ],
      "source": [
        "dataset_to_vector(train_df)\n",
        "dataset_to_vector(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train, y_train = train_df['sentence'], train_df['label']\n",
        "x_test, y_test = test_df['sentence'], test_df['label']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reshaping data for input into CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reshape(data):\n",
        "    rows = data.shape[0]\n",
        "    words = data[0].shape[0]\n",
        "    embed_dim = data[0].shape[1]\n",
        "\n",
        "    x = np.zeros(rows*words*embed_dim)\n",
        "    x = np.reshape(x,(rows,words,embed_dim))\n",
        "\n",
        "    for i in range(0,rows):\n",
        "        for j in range(words):\n",
        "            x[i][j] = data[i][j]\n",
        "        \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = reshape(x_train)\n",
        "x_test = reshape(x_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CNN Architecture that creates 32 features for each sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_features = Sequential()\n",
        "\n",
        "cnn_features.add(Conv1D(128, 1, strides=1, batch_input_shape=(None,8,300), padding='same'))\n",
        "cnn_features.add(BatchNormalization())\n",
        "cnn_features.add(Activation('relu'))\n",
        "cnn_features.add(Dropout(0.2))\n",
        "cnn_features.add(MaxPooling1D())\n",
        "\n",
        "cnn_features.add(Conv1D(64, 1, strides=1, padding='same'))\n",
        "cnn_features.add(BatchNormalization())\n",
        "cnn_features.add(Activation('relu'))\n",
        "cnn_features.add(Dropout(0.2))\n",
        "cnn_features.add(MaxPooling1D())\n",
        "\n",
        "cnn_features.add(Conv1D(32, 1, strides=1, padding='same'))\n",
        "cnn_features.add(BatchNormalization())\n",
        "cnn_features.add(Activation('relu'))\n",
        "cnn_features.add(Dropout(0.2))\n",
        "\n",
        "cnn_features.add(MaxPooling1D())\n",
        "cnn_features.add(Flatten())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import model_from_json\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ]
        }
      ],
      "source": [
        "json_file = open('metaphor/cnn_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "loaded_model.load_weights(\"metaphor/cnn_model.h5\")\n",
        "print(\"Loaded model from disk\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 9s 8ms/step\n",
            "2/2 [==============================] - 0s 78ms/step\n"
          ]
        }
      ],
      "source": [
        "X_train = loaded_model.predict(x_train)\n",
        "X_test = loaded_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "svc_model = SVC(C=1,kernel='rbf')\n",
        "svc_model.fit(X_train, y_train)\n",
        "pickle.dump(svc_model, open(\"metaphor/svc_model.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "svc_model = pickle.load(open(\"metaphor/svc_model.pkl\", 'rb'))\n",
        "svc_pred = svc_model.predict(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6984126984126984\n",
            "f1: 0.732394366197183\n",
            "Precision: 0.7428571428571429\n",
            "Recall: 0.7222222222222222\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, svc_pred))\n",
        "print(\"f1:\",metrics.f1_score(y_test, svc_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, svc_pred))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, svc_pred))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict output for sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pred_input(sentence):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(sentence)\n",
        "    filtered_sentence = [lemmatizer.lemmatize(w.lower()) for w in word_tokens if not w.lower() in stop_words and w.isalpha() and len(w)>2]\n",
        "    sentence_vector = text_to_vector(filtered_sentence)\n",
        "    sentence_vector = np.reshape(sentence_vector,(1,8,300))\n",
        "    x_sent = loaded_model.predict(sentence_vector)\n",
        "    y_sent_pred = svc_model.predict(x_sent)\n",
        "    return y_sent_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 52ms/step\n",
            "[1] Life is a highway\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "[0] Car on a highway\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "[0] Their faces were clouded with sadness.\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "[0] The container leaked gas\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "[0] stamp fruit extract the juice.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "[0] He leaked information\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "[1] We rotate crops\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "[1] lay a responsibility on someone.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "[1] The White House sits on Pennsylvania Avenue.\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "[1] The bicycle looped around the tree.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "[0] The earth is rotating on its axis\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "[0] She traced the circumstances of her birth.\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "[1] I can not digest all this information.\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "[0] The government floated the ruble for a few months.\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "[1] They taxed him failure to appear in court.\n"
          ]
        }
      ],
      "source": [
        "sent = '''Life is a highway\n",
        "Car on a highway\n",
        "Their faces were clouded with sadness.\n",
        "The container leaked gas\n",
        "stamp fruit extract the juice.\n",
        "He leaked information\n",
        "We rotate crops\n",
        "lay a responsibility on someone.\n",
        "The White House sits on Pennsylvania Avenue.\n",
        "The bicycle looped around the tree.\n",
        "The earth is rotating on its axis\n",
        "She traced the circumstances of her birth.\n",
        "I can not digest all this information.\n",
        "The government floated the ruble for a few months.\n",
        "They taxed him failure to appear in court.'''\n",
        "for sen in sent.split('\\n'):\n",
        "    print(pred_input(sen), sen)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
